{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example: Biclustering text documents\n",
    "\n",
    "\n",
    "This example is a slightly modified version of the [scikit-learn tutorial on spectral biclustering](http://scikit-learn.org/stable/auto_examples/bicluster/bicluster_newsgroups.html). \n",
    "\n",
    "It clusters documents and words in the [20 newsgroups datasets](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) using the [spectral co-clustering algorithm](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011). The dataset comprises around 10000 newsgroups posts on 20 topics such that the resulting document-word biclusters indicate subsets of words which are used more often in certain subsets of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"From: steve@hcrlgw (Steven Collins)\\nSubject: Sphere from 4 points\\nOrganization: Central Research Lab. Hitachi, Ltd.\\nLines: 24\\nNntp-Posting-Host: hcrlgw\\n\\n>\\n>Another method is to first find the center of the circle defined by 2 sets\\n>of 3 points, and intersecting the normals from there.  This would also define\\n>the circle center.  However, small numerical imprecisions would make the\\n>lines not intersect.  Supposedly 3 planes HAVE to intersect in a unique\\n>point if they are not parallel.\\n>\\n\\nHaving thought about this, why don't you project the 2 lines onto the 2d\\nplane formed by the lines.  Do an intersection calculation in the plane in\\n2D, where you're guaranteed a unique solution (unless they're parallel which\\nwon't happen in this case), and then use parametric distance along the lines\\nfrom the circle centres to determine the exact point of interest.  This\\nbypasses the messy error propogation required to do the calculation in 3d.\\n\\nHope I haven't put my foot in it again!\\n\\nsteve\\n---\\n-- \\n+---------------------------------------+--------------------------------+\\n| Steven Collins\\t\\t\\t| email: steve@crl.hitachi.co.jp |\\n| Visiting Computer Graphics Researcher\\t| phone: (0423)-23-1111 \\t |\\n| Hitachi Central Research Lab. Tokyo.\\t| fax:   (0423)-27-7742\\t\\t |\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.externals.six import iteritems\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "def number_aware_tokenizer(doc):\n",
    "    \"\"\" Tokenizer that maps all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    token_pattern = re.compile(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "    tokens = token_pattern.findall(doc)\n",
    "    tokens = [\"#NUMBER\" if token[0] in \"0123456789_\" else token\n",
    "              for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# exclude 'comp.os.ms-windows.misc'\n",
    "categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "newsgroups.data[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents will be represented using tf-idf (term frequencyâ€“inverse document frequency) vectorization. The result is a 10723x22217 sparse matrix (number of documents by number of words in the dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10723, 22217)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5,\n",
    "                             tokenizer=number_aware_tokenizer)\n",
    "\n",
    "cocluster = SpectralCoclustering(n_clusters=len(categories),\n",
    "                                 svd_method='arpack', random_state=0)\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coclustering...\n",
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 1995 documents, 4423 words\n",
      "categories   : 23% talk.politics.guns, 19% talk.politics.misc, 15% sci.med\n",
      "words        : gun, guns, geb, banks, firearms, gordon, clinton, cdt, surrender, veal\n",
      "\n",
      "bicluster 1 : 1183 documents, 3380 words\n",
      "categories   : 28% talk.politics.mideast, 26% soc.religion.christian, 25% alt.atheism\n",
      "words        : god, jesus, christians, atheists, kent, morality, sin, belief, objective, resurrection\n",
      "\n",
      "bicluster 2 : 2239 documents, 2829 words\n",
      "categories   : 18% comp.sys.mac.hardware, 16% comp.sys.ibm.pc.hardware, 16% comp.graphics\n",
      "words        : voltage, shipping, circuit, receiver, compression, stereo, hardware, package, processing, umass\n",
      "\n",
      "bicluster 3 : 1769 documents, 2661 words\n",
      "categories   : 26% rec.motorcycles, 23% rec.autos, 13% misc.forsale\n",
      "words        : bike, car, dod, ride, motorcycle, engine, bikes, bmw, honda, helmet\n",
      "\n",
      "bicluster 4 : 12 documents, 152 words\n",
      "categories   : 100% rec.sport.hockey\n",
      "words        : scorer, unassisted, reichel, semak, sweeney, kovalenko, ricci, audette, momesso, nedved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis], cols].sum() but\n",
    "    # much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(', '.join(important_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
